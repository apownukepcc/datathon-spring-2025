{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMc3CO945OUhVY9GcXDCjon"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWB_0xyri_T-","executionInfo":{"status":"ok","timestamp":1739806573150,"user_tz":420,"elapsed":69350,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"59d4980d-42a4-4510-da3a-3f9fad576514"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model for SO2TONS at LAKE-1:\n","  RMSE: 0.0073\n","  R²: -262337.1256\n","Model for SO2TONS at LAKE-2:\n","  RMSE: 0.0018\n","  R²: -19107.6562\n","Model for SO2TONS at LAKE-3:\n","  RMSE: 0.0021\n","  R²: -25220.1207\n","Model for SO2TONS at LAKE-4:\n","  RMSE: 0.0023\n","  R²: -25567.7081\n","Model for NOXTONS at LAKE-1:\n","  RMSE: 0.0087\n","  R²: -1140.7929\n","Model for NOXTONS at LAKE-2:\n","  RMSE: 0.0068\n","  R²: -949.5982\n","Model for NOXTONS at LAKE-3:\n","  RMSE: 0.0052\n","  R²: -509.9165\n","Model for NOXTONS at LAKE-4:\n","  RMSE: 0.0023\n","  R²: -110.8641\n","Model for COTONS at LAKE-1:\n","  RMSE: 0.0055\n","  R²: -3106.0307\n","Model for COTONS at LAKE-2:\n","  RMSE: 0.0021\n","  R²: -572.3720\n","Model for COTONS at LAKE-3:\n","  RMSE: 0.0009\n","  R²: -117.9118\n","Model for COTONS at LAKE-4:\n","  RMSE: 0.0039\n","  R²: -1367.3266\n","\n","Final Predictions:\n","SO2TONS at LAKE-1:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0000\n","  Predicted Emissions_Load: -0.0000\n","\n","SO2TONS at LAKE-2:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0000\n","  Predicted Emissions_Load: 0.0005\n","\n","SO2TONS at LAKE-3:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0000\n","  Predicted Emissions_Load: -0.0002\n","\n","SO2TONS at LAKE-4:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0000\n","  Predicted Emissions_Load: 0.0003\n","\n","NOXTONS at LAKE-1:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0008\n","  Predicted Emissions_Load: -0.0007\n","\n","NOXTONS at LAKE-2:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0008\n","  Predicted Emissions_Load: 0.0010\n","\n","NOXTONS at LAKE-3:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0005\n","  Predicted Emissions_Load: -0.0035\n","\n","NOXTONS at LAKE-4:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0006\n","  Predicted Emissions_Load: 0.0015\n","\n","COTONS at LAKE-1:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0001\n","  Predicted Emissions_Load: 0.0001\n","\n","COTONS at LAKE-2:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0002\n","  Predicted Emissions_Load: 0.0004\n","\n","COTONS at LAKE-3:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0001\n","  Predicted Emissions_Load: -0.0001\n","\n","COTONS at LAKE-4:\n","  Features: {'tavg': 31.7, 'tmin': 23.3, 'tmax': 38.9, 'prcp': 0.0, 'snow': 0.0, 'wdir': 87.0, 'wspd': 11.2, 'pres': 1011.3}\n","  Actual Emissions_Load: 0.0001\n","  Predicted Emissions_Load: 0.0046\n","\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","\n","# URLs for datasets\n","datasets = {\n","    \"SO2TONS\": \"https://raw.githubusercontent.com/apownukepcc/ForecastingDailyEmissions/refs/heads/main/SO2TONS_dataset.csv\",\n","    \"NOXTONS\": \"https://raw.githubusercontent.com/apownukepcc/ForecastingDailyEmissions/refs/heads/main/NOXTONS_dataset.csv\",\n","    \"COTONS\": \"https://raw.githubusercontent.com/apownukepcc/ForecastingDailyEmissions/refs/heads/main/COTONS_dataset.csv\"\n","}\n","\n","# Define the peak season months (May through August)\n","peak_season_months = [5, 6, 7, 8]\n","\n","# Define lakes (sources)\n","sources = [\"LAKE-1\", \"LAKE-2\", \"LAKE-3\", \"LAKE-4\"]\n","\n","# Define the specific day for prediction\n","specific_date = pd.Timestamp(\"2022-07-15\")\n","\n","# Define a PyTorch model\n","class EmissionPredictor(nn.Module):\n","    def __init__(self, input_size):\n","        super(EmissionPredictor, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 32)\n","        self.fc2 = nn.Linear(32, 16)\n","        self.fc3 = nn.Linear(16, 8)\n","        self.fc4 = nn.Linear(8, 1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.relu(self.fc3(x))\n","        x = self.fc4(x)\n","        return x\n","\n","# Initialize a dictionary to store models, predictions, and inputs for verification\n","models = {}\n","predictions = {}\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Loop through each dataset\n","for parameter, url in datasets.items():\n","    # Load the dataset\n","    data = pd.read_csv(url)\n","\n","    # Convert the 'date' column to datetime\n","    data['date'] = pd.to_datetime(data['date'])\n","\n","    # Filter for peak season\n","    data = data[data['date'].dt.month.isin(peak_season_months)]\n","\n","    # Separate data by source\n","    for source in sources:\n","        source_data = data[data['Source'] == source]\n","\n","        if source_data.empty or len(source_data) < 10:\n","            print(f\"Not enough data for {parameter} at {source}. Skipping...\")\n","            continue\n","\n","        # Define predictors and target\n","        predictors = ['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'pres']\n","        target = 'Emissions_Load'\n","\n","        # Drop rows with missing values\n","        source_data = source_data.dropna(subset=predictors + [target])\n","\n","        # Split the data into features (X) and target (y)\n","        X = source_data[predictors]\n","        y = source_data[target]\n","\n","        # Standardize features\n","        scaler = StandardScaler()\n","        X_scaled = scaler.fit_transform(X)\n","\n","        # Split into train and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","        # Convert to PyTorch tensors\n","        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n","        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n","        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n","        y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n","\n","        # Initialize the model\n","        model = EmissionPredictor(X_train.shape[1]).to(device)\n","        criterion = nn.MSELoss()\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        # Train the model\n","        epochs = 100\n","        batch_size = 8\n","\n","        for epoch in range(epochs):\n","            permutation = torch.randperm(X_train_tensor.size(0))\n","            for i in range(0, X_train_tensor.size(0), batch_size):\n","                indices = permutation[i:i+batch_size]\n","                batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n","\n","                optimizer.zero_grad()\n","                outputs = model(batch_x)\n","                loss = criterion(outputs, batch_y)\n","                loss.backward()\n","                optimizer.step()\n","\n","        # Evaluate the model\n","        with torch.no_grad():\n","            y_pred_tensor = model(X_test_tensor)\n","            y_pred = y_pred_tensor.cpu().numpy().flatten()\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        r2 = r2_score(y_test, y_pred)\n","\n","        print(f\"Model for {parameter} at {source}:\")\n","        print(f\"  RMSE: {rmse:.4f}\")\n","        print(f\"  R²: {r2:.4f}\")\n","\n","        models[(parameter, source)] = (model, scaler)\n","\n","        # Predict for specific date\n","        day_data = source_data[source_data['date'] == specific_date]\n","        if not day_data.empty:\n","            specific_features = scaler.transform(day_data[predictors])\n","            specific_features_tensor = torch.tensor(specific_features, dtype=torch.float32).to(device)\n","\n","            with torch.no_grad():\n","                specific_prediction = model(specific_features_tensor).cpu().numpy()[0, 0]\n","\n","            specific_actual = day_data[target].iloc[0]\n","\n","            predictions[(parameter, source)] = {\n","                \"features\": day_data[predictors].iloc[0],\n","                \"actual\": specific_actual,\n","                \"predicted\": specific_prediction\n","            }\n","\n","# Display all predictions at the end\n","print(\"\\nFinal Predictions:\")\n","for key, value in predictions.items():\n","    parameter, source = key\n","    print(f\"{parameter} at {source}:\")\n","    print(f\"  Features: {value['features'].to_dict()}\")\n","    print(f\"  Actual Emissions_Load: {value['actual']:.4f}\")\n","    print(f\"  Predicted Emissions_Load: {value['predicted']:.4f}\")\n","    print()\n"]}]}